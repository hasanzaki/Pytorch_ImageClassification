{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_Classification_using_pre_trained_models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"boYPyI0MdVTn"},"source":["# PyTorch for Beginners: Comparison of pre-trained models for Image Classification"]},{"cell_type":"code","metadata":{"id":"0SlHsDqPPtMi","colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"status":"error","timestamp":1608716201975,"user_tz":-480,"elapsed":661,"user":{"displayName":"HASAN FIRDAUS MOHD ZAKI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5YT9bdu_o0q4HovJ6XcpBGhkhoRB9nSuY7yX5_g=s64","userId":"00815015697344756688"}},"outputId":"76836b6c-242f-4e9b-e59e-1d2891d69a77"},"source":["import torch, torchvision\n","from torchvision import datasets, models, transforms"],"execution_count":7,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1aa0219b010f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshufflenetv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfaster_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmask_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkeypoint_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/faster_rcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmisc_nn_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiScaleRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_iou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mroi_align\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroi_align\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mroi_pool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroi_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRoIPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoolers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiScaleRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeature_pyramid_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeaturePyramidNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: libcudart.so.9.0: cannot open shared object file: No such file or directory","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"-d7vH5B2P02M","colab":{"base_uri":"https://localhost:8080/","height":1003},"outputId":"d6306d22-c0d0-4136-f22c-1ec72a67aaab"},"source":["# Print the models available in torchvision\n","dir(models)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['AlexNet',\n"," 'DenseNet',\n"," 'GoogLeNet',\n"," 'Inception3',\n"," 'MobileNetV2',\n"," 'ResNet',\n"," 'ShuffleNetV2',\n"," 'SqueezeNet',\n"," 'VGG',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_utils',\n"," 'alexnet',\n"," 'densenet',\n"," 'densenet121',\n"," 'densenet161',\n"," 'densenet169',\n"," 'densenet201',\n"," 'detection',\n"," 'googlenet',\n"," 'inception',\n"," 'inception_v3',\n"," 'mobilenet',\n"," 'mobilenet_v2',\n"," 'resnet',\n"," 'resnet101',\n"," 'resnet152',\n"," 'resnet18',\n"," 'resnet34',\n"," 'resnet50',\n"," 'resnext101_32x8d',\n"," 'resnext50_32x4d',\n"," 'segmentation',\n"," 'shufflenet_v2_x0_5',\n"," 'shufflenet_v2_x1_0',\n"," 'shufflenet_v2_x1_5',\n"," 'shufflenet_v2_x2_0',\n"," 'shufflenetv2',\n"," 'squeezenet',\n"," 'squeezenet1_0',\n"," 'squeezenet1_1',\n"," 'utils',\n"," 'vgg',\n"," 'vgg11',\n"," 'vgg11_bn',\n"," 'vgg13',\n"," 'vgg13_bn',\n"," 'vgg16',\n"," 'vgg16_bn',\n"," 'vgg19',\n"," 'vgg19_bn']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"QL1bqOU_Qd-t"},"source":["# Specify image transformations\n","from torchvision import transforms\n","\n","transform = transforms.Compose([            #[1]\n"," transforms.Resize(256),                    #[2]\n"," transforms.CenterCrop(224),                #[3]\n"," transforms.ToTensor(),                     #[4]\n"," transforms.Normalize(                      #[5]\n"," mean=[0.485, 0.456, 0.406],                #[6]\n"," std=[0.229, 0.224, 0.225]                  #[7]\n"," )])\n","\n","# Line [1]: Here we are defining a variable transform which is a combination of all the image transformations to be carried out on the input image.\n","\n","# Line [2]: Resize the image to 256×256 pixels.\n","\n","# Line [3]: Crop the image to 224×224 pixels about the center.\n","\n","# Line [4]: Convert the image to PyTorch Tensor data type.\n","\n","# Line [5-7]: Normalize the image by setting its mean and standard deviation to the specified values."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qtOK2ZLTwss","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"97457bf6-0d41-4dbf-9807-016c594fc998"},"source":["# Download image\n","!wget https://upload.wikimedia.org/wikipedia/commons/2/26/YellowLabradorLooking_new.jpg -O dog.jpg"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2019-06-03 03:46:41--  https://upload.wikimedia.org/wikipedia/commons/2/26/YellowLabradorLooking_new.jpg\n","Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:861:ed1a::2:b\n","Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 83281 (81K) [image/jpeg]\n","Saving to: ‘dog.jpg’\n","\n","dog.jpg             100%[===================>]  81.33K  --.-KB/s    in 0.04s   \n","\n","2019-06-03 03:46:46 (1.81 MB/s) - ‘dog.jpg’ saved [83281/83281]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G31tXCQxa1AZ","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"edd82d25-72ed-4c0a-afb2-d48e610adeed"},"source":["# Download classes text file\n","!wget https://raw.githubusercontent.com/Lasagne/Recipes/master/examples/resnet50/imagenet_classes.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2019-06-03 03:46:49--  https://raw.githubusercontent.com/Lasagne/Recipes/master/examples/resnet50/imagenet_classes.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 21674 (21K) [text/plain]\n","Saving to: ‘imagenet_classes.txt’\n","\n","\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  21.17K  --.-KB/s    in 0.007s  \n","\n","2019-06-03 03:46:49 (2.89 MB/s) - ‘imagenet_classes.txt’ saved [21674/21674]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jex0-5RuaNRP"},"source":["from PIL import Image\n","img = Image.open(\"dog.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbPGrVeRapB9"},"source":["img_t = transform(img)\n","batch_t = torch.unsqueeze(img_t, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HoQbsp8coVb","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d41d63e4-b47c-49e3-9aee-92b084093561"},"source":["# Load alexnet model\n","alexnet = models.alexnet(pretrained=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n","100%|██████████| 244418560/244418560 [00:05<00:00, 45622470.43it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FetOTbiTc72w","colab":{"base_uri":"https://localhost:8080/","height":476},"outputId":"e5bfa8bf-a555-42e0-fbf0-0d82cfcfcf02"},"source":["print(alexnet)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace)\n","    (3): Dropout(p=0.5)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pBc7fC2Hc-Xm","colab":{"base_uri":"https://localhost:8080/","height":476},"outputId":"79aa6483-dd26-455f-db37-0c44cc34b6e3"},"source":["# Put our model in eval mode\n","alexnet.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace)\n","    (3): Dropout(p=0.5)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"BtZvoeNkdCkI","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"582ef133-c73a-4702-faa4-b5c144085713"},"source":["# Carry out inference\n","out = alexnet(batch_t)\n","print(out.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 1000])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y5pqY-qXdDgv"},"source":["# Load labels\n","with open('imagenet_classes.txt') as f:\n","  classes = [line.strip() for line in f.readlines()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OATsity0dGoV","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"1e9cf967-f8d7-4468-ddb8-778042e1b7be"},"source":["_, indices = torch.sort(out, descending=True)\n","percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n","[(classes[idx], percentage[idx].item()) for idx in indices[0][:5]]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Labrador retriever', 41.585166931152344),\n"," ('golden retriever', 16.59166145324707),\n"," ('Saluki, gazelle hound', 16.286880493164062),\n"," ('whippet', 2.8539133071899414),\n"," ('Ibizan hound, Ibizan Podenco', 2.3924720287323)]"]},"metadata":{"tags":[]},"execution_count":17}]}]}